#!/usr/bin/env python3

import argparse
import os
from os.path import join, abspath, expanduser
import sys
import multiprocessing

try:
    import snakemake
except ImportError:
    print("Snakemake not found. Did you forget 'conda activate snakemake'? "
          "Alternatively, you may install Snakemake system-wide.", file=sys.stderr)
    exit(1)


def main(argv = sys.argv[1:]):

    p = argparse.ArgumentParser(
        description="amplicon data processing workflows for diversity analysis",
        usage="""snakecharmer.py [parameters] [<snakemake parameters>] <analysis directory> [target, ...]

Snakecharmer allows running and comparing amplicon analysis workflows.
This script is a more user friendly wrapper for snakemake adapted to this
specific use case. Additional options are directly passed on to snakemake.

Available rules:
* preparation: samples, unique_samples, config, quality
* main rules: cluster, taxonomy
* additional/special: cmp, ITS
* cleanup: clean, clean_all, clean_tax, clean_taxdb, clean_fastqc, clean_cmp, clean_itsx

Example:
Running on a local computer with 8 cores
(the 'outdir' directory must contain a 'config' directory with 'config.yaml' and 'taxonomy.yaml')

$ cd snakecharmer-main
$ conda activate snakemake
$ snakecharmer -c8 outdir cluster
""")
    p.add_argument(
        "directory",
        help="""
        Working directory where the configuration is found ('config'
        directory) and all output is placed.
        """)
    p.add_argument(
        "target",
        nargs="*",
        help="""
        Targets to build (rules or files).
        """)

    d = p.add_argument_group("Data storage")
    d.add_argument(
        "--conda-dir",
        default=join("~", "snakecharmer", "conda"),
        help="""
        Location to store Conda environments and archives
        """)
    d.add_argument(
        "--cache-dir",
        default=join("~", "snakecharmer", "cache"),
        help="""
        Location of the between-workflow cache, which is currently used
        to keep taxonomic databases without having to re-download them
        all the time.
        """)

    r = p.add_argument_group("job resources")
    r.add_argument(
        "-c", "--cores",
        default="1",
        help="""
        Number of CPU cores to use at most, or 'all' to use all available cores.
        The default is 1 (one core)
        """)
    r.add_argument(
        "-j", "--jobs",
        default="1",
        help="""
        Number of jobs to submit or 'unlimited'. This is only relevant in cluster/cloud mode.
        The default is 1, so make sure to change this in order to run jobs
        simultanesously on multiple nodes.
        """)
    r.add_argument(
        "--local-cores", type=int,
        default=1,
        help="""
        Number of cores to use for very short computations on the host machine.
        This is not relevant on a normal PC.
        """)
    
    dev = p.add_argument_group("Other")
    dev.add_argument(
        "--dev", action="store_true",
        help="""
        Developer mode: equivalent to --rerun-triggers mtime,params,input,software-env
        (excluding 'code' to make sure not everything is always re-run).
        Also, --quiet is not supplied, showing all output of snakemake
        """
    )
    dev.add_argument(
        "-v", "--verbose", action="store_true",
        help="Show all snakemake output (--quiet flag omitted)"
    )

    args, other_args = p.parse_known_args(argv)

    # set SNAKEMAKE_OUTPUT_CACHE for use with --cache
    cache_dir = abspath(expanduser(args.cache_dir))
    os.makedirs(cache_dir, exist_ok=True)
    os.environ["SNAKEMAKE_OUTPUT_CACHE"] = cache_dir
    # even though we supply --conda-prefix, we also define the env. var in order
    # to set the prefix for Snakemake subprocesses
    os.environ["SNAKEMAKE_CONDA_PREFIX"] = args.conda_dir

    # determine resources
    n_cores = multiprocessing.cpu_count() if args.cores == 'all' else int(args.cores)
    if args.jobs == 'unlimited':
        print("Warning: setting -j/--jobs to 'unlimited' can cause many"
              "small jobs to run. Setting to 20 jobs. This can be changed with -j <number>.",
               file=sys.stderr)
        n_jobs = 20
    else:
        n_jobs = int(args.jobs)

    # run Snakemake
    cmd = [
        "-d", args.directory,
        "-c", str(n_cores), "-j", str(n_jobs),
        "--local-cores", str(args.local_cores),
        "--use-conda", "--conda-prefix", args.conda_dir,
        "--cache",
        "--rerun-incomplete"
    ]
    cmd = cmd + other_args + args.target

    if args.dev:
        cmd += ["--rerun-triggers", "mtime", "params", "input", "software-env"]
    elif not args.verbose:
        cmd += ["--quiet", "rules"]
    
    print(f"Running: SNAKEMAKE_OUTPUT_CACHE={cache_dir} snakemake " + " ".join(cmd), file=sys.stderr)
    snakemake.main(cmd)


if __name__ == '__main__':
    main()
