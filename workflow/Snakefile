from snakemake.utils import min_version
from snakemake.utils import validate

##### set minimum snakemake version #####

min_version("6.1.4")

##### configuration #####


configfile: "config/config.yaml"
configfile: "config/taxonomy.yaml"


validate(config, "config.schema.yaml")

##### load rules #####


include: "rules/common.smk"
include: "rules/qc.smk"

if "usearch" in config["software"]:
    include: "rules/usearch.smk"

if "qiime" in config["software"]:
    include: "rules/qiime.smk"

if "amptk" in config["software"]:
    include: "rules/amptk.smk"


include: "rules/taxonomy.smk"


include: "rules/other.smk"


#### Paths ####


results = lambda sub_path: list(
    chain(
        *[
            expand_runs(
                "results/{workflow}/workflow_{cluster}/{run}_{layout}/{marker}__{primers}/{sub_path}",
                workflow=name,
                cluster=p["cluster"],
                marker=marker,
                primers=primer_comb,
                sub_path=sub_path,
                pool=p["settings"]["pool_raw"],
                #allow_missing=True,
            )
            for name, p in cfg.workflows.items()
            for marker, primer_comb in cfg.primer_combinations.items()
        ]
    )
)


taxdirs = lambda sub_path, ext: list(
    chain(
        *[
            expand_runs(
                "results/{workflow}/workflow_{cluster}/{run}_{layout}/{marker}__{primers}/{sub_path}/{tax[db_name]}-{tax[assign][method]}-{tax[method_name]}.{ext}",
                workflow=name,
                cluster=p["cluster"],
                marker=marker,
                primers=primer_comb,
                sub_path=sub_path,
                tax=p["taxonomy"][marker].values(),
                ext=ext,
                pool=p["settings"]["pool_raw"],
                # allow_missing=True,
            )
            for name, p in cfg.workflows.items()
            for marker, primer_comb in cfg.primer_combinations.items()
        ]
    )
)


#### target rules #####

from itertools import chain


rule config:
    input:
        expand("results/{workflow}/config.yaml", workflow=cfg.workflows),


rule samples:
    input:
        expand("results/{workflow}/samples.yaml", workflow=cfg.workflows),


rule unique_samples:
    input:
        rules.collect_unique_files.output,


rule quality:
    input:
        expand("results/{workflow}/validation/multiqc/multiqc_report.html", workflow=cfg.workflows),
        expand("results/{workflow}/validation/multiqc_{cluster}/multiqc_report.html",
               zip, workflow=cfg.workflows,
               cluster=[w["cluster"].split('_', 1)[0] for w in cfg.workflows.values()],
               allow_missing=True)
        # rules.config.input,


rule denoise:
    input:
        rules.config.input,
        rules.samples.input,
        rules.quality.input,
        lambda _: results((
            "denoised.fasta",
            "denoised_otutab.txt.gz",
            "denoised.biom",
        )),
        expand("results/{workflow}/sample_report.tsv", workflow=cfg.workflows),
        expand("results/{workflow}/.outdirs", workflow=cfg.workflows, allow_missing=True),


rule taxonomy:
    input:
        lambda _: taxdirs("taxonomy", ("txt.gz", "biom.gz")),
        lambda _: taxdirs("taxonomy/fasta", ("fasta.gz"))

rule ITS:
    input:
        lambda _: results("ITSx/out.positions.txt"),


rule cmp:
    input:
        lambda _: results(
            expand("cmp/{db}.{ext}", db=cfg.cmp_files, ext=("txt", "bam"))
        ),



# commands for cleaning up


localrules:
    clean,
    clean_all,


rule clean:
    shell:
        "rm -Rf input processing logs"


rule clean_all:
    shell:
        "rm -Rf results input unique_samples processing logs refdb"
