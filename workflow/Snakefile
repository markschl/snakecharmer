from snakemake.utils import min_version

##### set minimum snakemake version #####

min_version("6.1.4")

##### configuration #####


configfile: "config/config.yaml"
configfile: "config/taxonomy.yaml"


##### load rules #####


include: "rules/common.smk"
include: "rules/usearch.smk"
if "qiime" in config["software"]:
    include: "rules/qiime.smk"
if "amptk" in config["software"]:
    include: "rules/amptk.smk"
include: "rules/taxonomy.smk"
include: "rules/other.smk"


#### Paths ####

pipeline_dirs = expand(
    "results/{name}/pipeline_{cluster}",
    zip,
    name=cfg.pipelines.keys(),
    cluster=(p["cluster"] for p in cfg.pipelines.values()),
)

simple_results_dirs = expand(
    "results/{name}/data",
    name=(name for name, p in cfg.pipelines.items() if p["is_simple"]),
)

results_dirs = expand(
    "{prefix}/{primers}/{strategy}",
    prefix=pipeline_dirs,
    primers=cfg.primer_combinations_flat,
    strategy=cfg.sequencing_strategies,
)


taxonomy_prefixes = lambda sub_path: list(
    chain(
        *[
            expand(
                "results/{name}/pipeline_{cluster}/{marker}__{primers}/{strategy}/{sub_path}/{tax[db_name]}-{tax[method]}-{tax[method_name]}",
                name=name,
                cluster=p["cluster"],
                strategy=p["sequencing_strategies"],
                marker=marker,
                primers=primer_comb,
                sub_path=sub_path,
                tax=p["taxonomy"][marker].values(),
            )
            for name, p in cfg.pipelines.items()
            for marker, primer_comb in cfg.primer_combinations.items()
        ]
    )
)


#### target rules #####

from itertools import chain


rule config:
    input:
        rules.dump_samples.output,
        expand("results/{name}/config.yaml", name=cfg.pipelines),


rule quality:
    input:
        rules.multiqc_fastqc.output,
        rules.config.input,


rule denoise:
    input:
        rules.config.input,
        simple_results_dirs,
        expand("{prefix}/denoised.fasta", prefix=results_dirs),
        expand("{prefix}/denoised_otutab.txt.gz", prefix=results_dirs),
        expand("{prefix}/denoised.biom", prefix=results_dirs),
        expand("{prefix}/_validation/multiqc/multiqc_report.html", prefix=pipeline_dirs),
        # TODO: not all pipelines produce a report
        expand(
            "{prefix}/_validation/sample_report.tsv",
            prefix=pipeline_dirs,
            allow_missing=True,
        ),


rule taxonomy:
    input:
        expand("{prefix}.txt.gz", prefix=taxonomy_prefixes("taxonomy")),
        expand("{prefix}.biom.gz", prefix=taxonomy_prefixes("taxonomy")),
        expand("{prefix}.fasta.gz", prefix=taxonomy_prefixes("taxonomy/fasta")),


rule ITS:
    input:
        expand("{prefix}/ITSx/out.positions.txt", prefix=results_dirs),


rule cmp:
    input:
        expand("{prefix}/cmp/{db}.txt", prefix=results_dirs, db=cfg.cmp_files),
        expand("{prefix}/cmp/{db}.bam", prefix=results_dirs, db=cfg.cmp_files),


# commands for cleaning up

rule clean:
    shell:
        "rm -Rf input processing"


rule clean_all:
    shell:
        "rm -Rf results input processing logs"


rule clean_taxdb:
    shell:
        "rm -Rf refdb"


rule clean_tax:
    shell:
        "rm -Rf results/*/pipeline_*/*/*/taxonomy"


rule clean_itsx:
    shell:
        "rm -Rf results/*/pipeline_*/*/*/ITSx"


rule clean_cmp:
    shell:
        "rm -Rf results/*/pipeline_*/*/*/cmp"
