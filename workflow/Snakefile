from snakemake.utils import min_version
from snakemake.utils import validate

##### set minimum snakemake version #####

min_version("6.1.4")

##### configuration #####


configfile: "config/config.yaml"
configfile: "config/taxonomy.yaml"


validate(config, "config.schema.yaml")

##### load rules #####


include: "rules/common.smk"
include: "rules/qc.smk"
include: "rules/usearch.smk"


if "qiime" in config["software"]:

    include: "rules/qiime.smk"


if "amptk" in config["software"]:

    include: "rules/amptk.smk"


include: "rules/taxonomy.smk"


include: "rules/other.smk"


#### Paths ####


pipeline_dirs = expand(
    "results/{name}/pipeline_{cluster}",
    zip,
    name=cfg.pipelines.keys(),
    cluster=(p["cluster"] for p in cfg.pipelines.values()),
)

simple_results_dirs = expand(
    "results/{name}/data",
    name=(name for name, p in cfg.pipelines.items() if p["is_simple"]),
)

results_dirs = expand(
    "{prefix}/{primers}/{strategy}",
    prefix=pipeline_dirs,
    primers=cfg.primer_combinations_flat,
    strategy=cfg.sequencing_strategies,
)


taxonomy_prefixes = lambda sub_path: list(
    chain(
        *[
            expand(
                "results/{name}/pipeline_{cluster}/{marker}__{primers}/{strategy}/{sub_path}/{tax[db_name]}-{tax[method]}-{tax[method_name]}",
                name=name,
                cluster=p["cluster"],
                strategy=p["sequencing_strategies"],
                marker=marker,
                primers=primer_comb,
                sub_path=sub_path,
                tax=p["taxonomy"][marker].values(),
            )
            for name, p in cfg.pipelines.items()
            for marker, primer_comb in cfg.primer_combinations.items()
        ]
    )
)


#### target rules #####

from itertools import chain


rule config:
    input:
        rules.dump_samples_yaml.output,
        expand("results/{name}/config.yaml", name=cfg.pipelines),


rule samples:
    input:
        rules.dump_samples_yaml.output,
        rules.dump_samples_tsv.output,
        rules.link_samples.output,


rule quality:
    input:
        rules.multiqc_fastqc.output,
        rules.config.input,


rule denoise:
    input:
        rules.config.input,
        rules.dump_samples_yaml.output,
        rules.dump_samples_tsv.output,
        simple_results_dirs,
        expand("{prefix}/denoised.fasta", prefix=results_dirs),
        expand("{prefix}/denoised_otutab.txt.gz", prefix=results_dirs),
        expand("{prefix}/denoised.biom", prefix=results_dirs),
        expand("{prefix}/_validation/multiqc/multiqc_report.html", prefix=pipeline_dirs),
        # TODO: not all pipelines produce a report
        expand(
            "{prefix}/_validation/sample_report.tsv",
            prefix=pipeline_dirs,
            allow_missing=True,
        ),


rule taxonomy:
    input:
        expand("{prefix}.txt.gz", prefix=taxonomy_prefixes("taxonomy")),
        expand("{prefix}.biom.gz", prefix=taxonomy_prefixes("taxonomy")),
        expand("{prefix}.fasta.gz", prefix=taxonomy_prefixes("taxonomy/fasta")),


rule ITS:
    input:
        expand("{prefix}/ITSx/out.positions.txt", prefix=results_dirs),


rule cmp:
    input:
        expand("{prefix}/cmp/{db}.txt", prefix=results_dirs, db=cfg.cmp_files),
        expand("{prefix}/cmp/{db}.bam", prefix=results_dirs, db=cfg.cmp_files),


# commands for cleaning up


localrules:
    clean,
    clean_all,


rule clean:
    shell:
        "rm -Rf input/grouped processing"


rule clean_all:
    shell:
        "rm -Rf results input/grouped processing logs"
